{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../..\")\n",
    "base_path = os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length, col, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path =  \"/00_input/data/divisions.json\"\n",
    "files_path = base_path + \"/00_input/_profiles_txt/file-0*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = spark.read.text(\"file://\" + files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = data_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def count_text(text):\n",
    "    return len(text) - text.count(\" \")\n",
    "\n",
    "def clean_text(text):\n",
    "    return \"\".join([char for char in text if char.isalnum() or char == \" \"]).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd['text_len'] = data_pd['value'].apply(lambda x: count_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd['clear_text'] = data_pd['value'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>text_len</th>\n",
       "      <th>clear_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Professional Summary</td>\n",
       "      <td>19</td>\n",
       "      <td>professional summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39763</td>\n",
       "      <td>Technical Skills</td>\n",
       "      <td>15</td>\n",
       "      <td>technical skills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39764</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39765</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39766</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39767</td>\n",
       "      <td>Programming Languages, Server-Side Scripting, ...</td>\n",
       "      <td>161</td>\n",
       "      <td>programming languages serverside scripting dat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39768 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   value  text_len  \\\n",
       "0                                                                0   \n",
       "1                                                                0   \n",
       "2                                   Professional Summary        19   \n",
       "3                                                                0   \n",
       "4                                                                0   \n",
       "...                                                  ...       ...   \n",
       "39763                                   Technical Skills        15   \n",
       "39764                                                            0   \n",
       "39765                                                            0   \n",
       "39766                                                            0   \n",
       "39767  Programming Languages, Server-Side Scripting, ...       161   \n",
       "\n",
       "                                              clear_text  \n",
       "0                                                         \n",
       "1                                                         \n",
       "2                                   professional summary  \n",
       "3                                                         \n",
       "4                                                         \n",
       "...                                                  ...  \n",
       "39763                                   technical skills  \n",
       "39764                                                     \n",
       "39765                                                     \n",
       "39766                                                     \n",
       "39767  programming languages serverside scripting dat...  \n",
       "\n",
       "[39768 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pd = data_pd[data_pd[\"text_len\"] > 20].drop_duplicates(subset=\"clear_text\", keep=\"first\", inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>text_len</th>\n",
       "      <th>clear_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10 years of experience in data engineering and...</td>\n",
       "      <td>60</td>\n",
       "      <td>10 years of experience in data engineering and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Hadoop and RDBMS data pipelines, transformatio...</td>\n",
       "      <td>65</td>\n",
       "      <td>hadoop and rdbms data pipelines transformation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>a variety of data processing and transformatio...</td>\n",
       "      <td>60</td>\n",
       "      <td>a variety of data processing and transformatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>▪ Performance tune Hadoop data systems and pi...</td>\n",
       "      <td>58</td>\n",
       "      <td>performance tune hadoop data systems and pip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>system configuration and processing using d...</td>\n",
       "      <td>62</td>\n",
       "      <td>system configuration and processing using d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39632</td>\n",
       "      <td>If you have any issues with OneDrive access, t...</td>\n",
       "      <td>83</td>\n",
       "      <td>if you have any issues with onedrive access te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39636</td>\n",
       "      <td>Final Profile Review Session:</td>\n",
       "      <td>26</td>\n",
       "      <td>final profile review session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39638</td>\n",
       "      <td>AFTER thorough study and review, you should ha...</td>\n",
       "      <td>86</td>\n",
       "      <td>after thorough study and review you should hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39640</td>\n",
       "      <td>THEN you will have one FINAL PROFILE REVIEW se...</td>\n",
       "      <td>85</td>\n",
       "      <td>then you will have one final profile review se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39767</td>\n",
       "      <td>Programming Languages, Server-Side Scripting, ...</td>\n",
       "      <td>161</td>\n",
       "      <td>programming languages serverside scripting dat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8797 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   value  text_len  \\\n",
       "5      10 years of experience in data engineering and...        60   \n",
       "6      Hadoop and RDBMS data pipelines, transformatio...        65   \n",
       "7      a variety of data processing and transformatio...        60   \n",
       "11      ▪ Performance tune Hadoop data systems and pi...        58   \n",
       "12        system configuration and processing using d...        62   \n",
       "...                                                  ...       ...   \n",
       "39632  If you have any issues with OneDrive access, t...        83   \n",
       "39636                      Final Profile Review Session:        26   \n",
       "39638  AFTER thorough study and review, you should ha...        86   \n",
       "39640  THEN you will have one FINAL PROFILE REVIEW se...        85   \n",
       "39767  Programming Languages, Server-Side Scripting, ...       161   \n",
       "\n",
       "                                              clear_text  \n",
       "5      10 years of experience in data engineering and...  \n",
       "6      hadoop and rdbms data pipelines transformation...  \n",
       "7      a variety of data processing and transformatio...  \n",
       "11       performance tune hadoop data systems and pip...  \n",
       "12        system configuration and processing using d...  \n",
       "...                                                  ...  \n",
       "39632  if you have any issues with onedrive access te...  \n",
       "39636                       final profile review session  \n",
       "39638  after thorough study and review you should hav...  \n",
       "39640  then you will have one final profile review se...  \n",
       "39767  programming languages serverside scripting dat...  \n",
       "\n",
       "[8797 rows x 3 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pd = filtered_pd[\"clear_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5        10 years of experience in data engineering and...\n",
       "6        hadoop and rdbms data pipelines transformation...\n",
       "7        a variety of data processing and transformatio...\n",
       "11         performance tune hadoop data systems and pip...\n",
       "12          system configuration and processing using d...\n",
       "                               ...                        \n",
       "39632    if you have any issues with onedrive access te...\n",
       "39636                         final profile review session\n",
       "39638    after thorough study and review you should hav...\n",
       "39640    then you will have one final profile review se...\n",
       "39767    programming languages serverside scripting dat...\n",
       "Name: clear_text, Length: 8797, dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.createDataFrame(filtered_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|clear_text                                                               |\n",
      "+-------------------------------------------------------------------------+\n",
      "|10 years of experience in data engineering and analytics working with    |\n",
      "|hadoop and rdbms data pipelines transformation and cleansing  skilled in |\n",
      "|a variety of data processing and transformation tools and data storage   |\n",
      "|  performance tune hadoop data systems and pipelines with optimized      |\n",
      "|   system configuration and processing using data processing tools hadoop|\n",
      "|   cycle apache camel apache flume kafka apatar clover and others        |\n",
      "|  worked with systems employing hive rdds dataframes                     |\n",
      "|  forensic analysis with large complex data sets using realtime          |\n",
      "|   analytics and distributed big data platforms                          |\n",
      "|  comfortable working with hadoop distributions cloudera cloudera        |\n",
      "|   impala and hortonworks                                                |\n",
      "|  strong in planning and development of data governance security and     |\n",
      "|  deep knowledge in incremental imports partitioning and bucketing       |\n",
      "|   concepts in hive and spark sql needed for optimization                |\n",
      "|  experience collecting log data from various sources using various file |\n",
      "|   types such as sql rdbms data lake data storage data mining            |\n",
      "|  manages integration of files of various file types into hdfs using     |\n",
      "|  skilled at staging data in hdfs for further analysis                   |\n",
      "|  collection of log data from different sources like web server logs and |\n",
      "|   social media data from facebook and twitter using flume and storing in|\n",
      "+-------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_lines = new_df.select(\"clear_text\")\n",
    "profile_lines.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "|clear_text                                                            |\n",
      "+----------------------------------------------------------------------+\n",
      "|  forensic analysis with large complex data sets using realtime       |\n",
      "|     skilled in data analysis and forensic analysis using hadoop tools|\n",
      "|     skilled in forensic methods of data cleaning and refining data   |\n",
      "|     internal forensic incident response and threat hunting using sans|\n",
      "|candidate  gcfe  giac forensic examiner certification                 |\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_lines.filter(col(\"clear_text\").contains(\"forensic\")).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|clear_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|windows active directory windows   tez zookeeper apache airflows                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|     replicated deployed kinesistoredshift capability in apache airflow                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|      for airflow redshift project                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|other  jdk  maven  streamsets  airflow  confluence  jira  pyspark                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|apache hue apache sqoop spark storm pig hive hdfs zookeeper tez oozie maven ant hcatalog apache drill presto airflow camel                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|apache ant apache cassandra apache flume apache hadoop apache hadoop yarn apache hbase apache hcatalog apache hive apache kafka apache maven apache oozie apache pig apache spark spark streaming spark mllib graphx scipy pandas rdds dataframes datasets mesos apache tez apache zookeeper cloudera impala hdfs hortonworks apache airflow and camel apache lucene kibana xpack apache solr apache drill presto apache hue sqoop kibana tableau aws cloud foundry                                                                                                    |\n",
      "|zookeeper cloudera impala hdfs hortonworks apache airflow and camel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|      airflow and camel apache hue yarn apache hive apache kafka apache                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|apache airflow apache camel apache flinkstratosphere hive pig sqoop flume scala python                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|apache ant apache cassandra apache flume apache hadoop apache hadoop yarn apache hbase apache hcatalog apache hive apache kafka apache maven apache oozie apache pig apache spark spark streaming spark mllib graphx scipy pandas rdds dataframes datasets mesos apache tez apache zookeeper cloudera impala hdfs hortonworks apache airflow and camel apache lucene elasticsearch elastic cloud kibana xpack apache solr apache drill presto apache hue sqoop kibana tableau aws cloud foundry aws azure anaconda cloud elasticsearch solr lucene databricks mapreduce|\n",
      "|airflow used my personal laptop to learn airflow and stand up a test cluster before pushing forward with a request to install it on cp                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|wrote dags and tested airflow to verify based on business requirements                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|wrote up requirements document for third party infogroup to install and set up airflow                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|work on sqssensor and s3keysensors for airflow                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|environment hive spark pyspark emr mapr s3 airflow nltk pandas sql aws cli                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|apache ant apache cassandra apache flume apache hadoop apache hadoop yarn apache hbase apache hcatalog apache hive apache kafka apache maven apache oozie apache pig apache spark spark streaming spark mllib graphx scipy pandas rdds dataframes datasets mesos apache tez apache zookeeper cloudera impala hdfs hortonworks mapr mapreduce apache airflow and camel apache lucene elasticsearch elastic cloud kibana xpack apache solr apache drill presto apache hue sqoop kibana tableau aws cloud foundry github bit bucket pentaho kettle                        |\n",
      "|solr apache airflow apache camel spark                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|apache airflow and camel apache lucene elasticsearch elastic cloud                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|kibana tableau sqoop apache drill presto apache flume apache airflow and camel apache hue yarn apache hive apache kafka apache maven apache oozie apache pig apache spark spark streaming                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|the problem was lack of functionality within the enterprise cloud platform to run automated jobs we needed to enable the data scientists to run schedulable and repeatable jobs on a massive scale both in terms of population and compute in a google cloudmicrosoft azure environment to solve the problem i created a custom solution by way of a web application using apache airflow                                                                                                                                                                              |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_lines.filter(col(\"clear_text\").contains(\"airflow\")).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
