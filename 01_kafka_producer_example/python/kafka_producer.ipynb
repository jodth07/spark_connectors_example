{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kafka-server-start.sh config/server.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic topic_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kafka-console-producer.sh --broker-list localhost:9092 --topic topic_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/opt/kafka/logs/broker-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_SERVER = '10.0.0.9:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoBrokersAvailable",
     "evalue": "NoBrokersAvailable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoBrokersAvailable\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-e6b29c6e6fdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m producer = KafkaProducer(bootstrap_servers=['10.0.0.9:9092'],\n\u001b[1;32m----> 2\u001b[1;33m                          value_serializer=lambda data: dumps(data).encode('utf-8'))\n\u001b[0m",
      "\u001b[1;32mc:\\users\\jodth\\.conda\\envs\\spark_example\\lib\\site-packages\\kafka\\producer\\kafka.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **configs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         client = KafkaClient(metrics=self._metrics, metric_group_prefix='producer',\n\u001b[0;32m    381\u001b[0m                              \u001b[0mwakeup_timeout_ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_block_ms'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m                              **self.config)\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;31m# Get auto-discovered version from client if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jodth\\.conda\\envs\\spark_example\\lib\\site-packages\\kafka\\client_async.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **configs)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'api_version'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[0mcheck_timeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'api_version_auto_timeout_ms'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'api_version'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_can_bootstrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jodth\\.conda\\envs\\spark_example\\lib\\site-packages\\kafka\\client_async.py\u001b[0m in \u001b[0;36mcheck_version\u001b[1;34m(self, node_id, timeout, strict)\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoBrokersAvailable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwakeup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNoBrokersAvailable\u001b[0m: NoBrokersAvailable"
     ]
    }
   ],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=['10.0.0.9:9092'],\n",
    "                         value_serializer=lambda data: dumps(data).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(10):\n",
    "    data = {\n",
    "        \"id\": e,\n",
    "        \"link\":'http://trefle.io/api/subkingdoms/1', \n",
    "        \"name\":'Tracheobionta', \n",
    "        \"slug\":'tracheobionta'\n",
    "    }\n",
    "    producer.send('topic_one', value=data)\n",
    "    sleep(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Sample Log Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = ['PySpark Structured Streaming with Kafka',\n",
    "'Ivy Default Cache set to: /Users/yashiro/.ivy2/cache',\n",
    "'The jars for the packages stored in: /Users/yashiro/.ivy2/jars',\n",
    "':: loading settings :: url = jar:file:/opt/spark-2.4.5-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml',\n",
    "'com.datastax.spark#spark-cassandra-connector_2.11 added as a dependency',\n",
    "':: resolving dependencies :: org.apache.spark#spark-submit-parent-c122f657-bad9-405f-87b4-89a7dc1ef8e7;1.0',\n",
    "'confs: [default]',\n",
    "'found com.datastax.spark#spark-cassandra-connector_2.11;2.5.0 in spark-list',\n",
    "'found com.datastax.spark#spark-cassandra-connector-driver_2.11;2.5.0 in spark-list',\n",
    "'found com.datastax.oss#java-driver-core-shaded;4.5.0 in spark-list',\n",
    "'found com.datastax.oss#native-protocol;1.4.9 in spark-list',\n",
    "'found com.datastax.oss#java-driver-shaded-guava;25.1-jre in spark-list',\n",
    "'found com.typesafe#config;1.3.4 in spark-list',\n",
    "'found com.github.jnr#jnr-ffi;2.1.10 in spark-list',\n",
    "'found com.github.jnr#jffi;1.2.19 in spark-list',\n",
    "'found org.ow2.asm#asm;7.1 in spark-list',\n",
    "'found org.ow2.asm#asm-commons;7.1 in spark-list',\n",
    "'found org.ow2.asm#asm-tree;7.1 in spark-list',\n",
    "'found org.ow2.asm#asm-analysis;7.1 in spark-list',\n",
    "'found org.ow2.asm#asm-util;7.1 in spark-list',\n",
    "'found com.github.jnr#jnr-a64asm;1.0.0 in spark-list',\n",
    "'found com.github.jnr#jnr-x86asm;1.0.2 in spark-list',\n",
    "'found com.github.jnr#jnr-posix;3.0.50 in spark-list',\n",
    "'found com.github.jnr#jnr-constants;0.9.12 in spark-list',\n",
    "'found org.slf4j#slf4j-api;1.7.26 in spark-list',\n",
    "'found io.dropwizard.metrics#metrics-core;4.0.5 in spark-list',\n",
    "'found org.hdrhistogram#HdrHistogram;2.1.11 in spark-list',\n",
    "'found org.apache.tinkerpop#gremlin-core;3.4.5 in spark-list',\n",
    "'found org.apache.tinkerpop#gremlin-shaded;3.4.5 in spark-list',\n",
    "'found commons-configuration#commons-configuration;1.10 in spark-list',\n",
    "'found commons-lang#commons-lang;2.6 in spark-list',\n",
    "'found commons-collections#commons-collections;3.2.2 in spark-list',\n",
    "'found org.yaml#snakeyaml;1.15 in spark-list',\n",
    "'found org.javatuples#javatuples;1.2 in spark-list',\n",
    "'found com.carrotsearch#hppc;0.7.1 in spark-list',\n",
    "'found com.jcabi#jcabi-manifests;1.1 in spark-list',\n",
    "'found com.jcabi#jcabi-log;0.14 in spark-list',\n",
    "'found com.squareup#javapoet;1.11.1 in central',\n",
    "'found net.objecthunter#exp4j;0.4.8 in spark-list',\n",
    "'found org.slf4j#jcl-over-slf4j;1.7.25 in spark-list',\n",
    "'found org.apache.tinkerpop#gremlin-driver;3.4.5 in spark-list',\n",
    "'found org.codehaus.groovy#groovy;2.5.7 in spark-list',\n",
    "'found org.codehaus.groovy#groovy-json;2.5.7 in spark-list',\n",
    "'found org.apache.tinkerpop#tinkergraph-gremlin;3.4.5 in spark-list',\n",
    "'found org.reactivestreams#reactive-streams;1.0.2 in spark-list',\n",
    "'found com.github.stephenc.jcip#jcip-annotations;1.0-1 in spark-list',\n",
    "'found com.github.spotbugs#spotbugs-annotations;3.1.12 in spark-list',\n",
    "'found com.google.code.findbugs#jsr305;3.0.2 in spark-list',\n",
    "'found com.datastax.oss#java-driver-mapper-runtime;4.5.0 in spark-list',\n",
    "'found com.datastax.oss#java-driver-query-builder;4.5.0 in spark-list',\n",
    "'found org.apache.commons#commons-lang3;3.5 in spark-list',\n",
    "'found com.thoughtworks.paranamer#paranamer;2.8 in spark-list',\n",
    "'found com.typesafe.scala-logging#scala-logging_2.11;3.5.0 in spark-list',\n",
    "'found org.scala-lang#scala-reflect;2.11.12 in central',\n",
    "':: resolution report :: resolve 758ms :: artifacts dl 16ms',\n",
    "':: modules in use:',\n",
    "'com.carrotsearch#hppc;0.7.1 from spark-list in [default]',\n",
    "'com.datastax.oss#java-driver-core-shaded;4.5.0 from spark-list in [default]',\n",
    "'com.datastax.oss#java-driver-mapper-runtime;4.5.0 from spark-list in [default]',\n",
    "'com.datastax.oss#java-driver-query-builder;4.5.0 from spark-list in [default]',\n",
    "'com.datastax.oss#java-driver-shaded-guava;25.1-jre from spark-list in [default]',\n",
    "'com.datastax.oss#native-protocol;1.4.9 from spark-list in [default]',\n",
    "'com.datastax.spark#spark-cassandra-connector-driver_2.11;2.5.0 from spark-list in [default]',\n",
    "'com.datastax.spark#spark-cassandra-connector_2.11;2.5.0 from spark-list in [default]',\n",
    "'com.github.jnr#jffi;1.2.19 from spark-list in [default]',\n",
    "'com.github.jnr#jnr-a64asm;1.0.0 from spark-list in [default]',\n",
    "'com.github.jnr#jnr-constants;0.9.12 from spark-list in [default]',\n",
    "'com.github.jnr#jnr-ffi;2.1.10 from spark-list in [default]',\n",
    "'com.github.jnr#jnr-posix;3.0.50 from spark-list in [default]',\n",
    "'com.github.jnr#jnr-x86asm;1.0.2 from spark-list in [default]',\n",
    "'com.github.spotbugs#spotbugs-annotations;3.1.12 from spark-list in [default]',\n",
    "'com.github.stephenc.jcip#jcip-annotations;1.0-1 from spark-list in [default]',\n",
    "'com.google.code.findbugs#jsr305;3.0.2 from spark-list in [default]',\n",
    "'com.jcabi#jcabi-log;0.14 from spark-list in [default]',\n",
    "'com.jcabi#jcabi-manifests;1.1 from spark-list in [default]',\n",
    "'com.squareup#javapoet;1.11.1 from central in [default]',\n",
    "'com.thoughtworks.paranamer#paranamer;2.8 from spark-list in [default]',\n",
    "'com.typesafe#config;1.3.4 from spark-list in [default]',\n",
    "'com.typesafe.scala-logging#scala-logging_2.11;3.5.0 from spark-list in [default]',\n",
    "'commons-collections#commons-collections;3.2.2 from spark-list in [default]',\n",
    "'commons-configuration#commons-configuration;1.10 from spark-list in [default]',\n",
    "'commons-lang#commons-lang;2.6 from spark-list in [default]',\n",
    "'io.dropwizard.metrics#metrics-core;4.0.5 from spark-list in [default]',\n",
    "'net.objecthunter#exp4j;0.4.8 from spark-list in [default]',\n",
    "'org.apache.commons#commons-lang3;3.5 from spark-list in [default]',\n",
    "'org.apache.tinkerpop#gremlin-core;3.4.5 from spark-list in [default]',\n",
    "'org.apache.tinkerpop#gremlin-driver;3.4.5 from spark-list in [default]',\n",
    "'org.apache.tinkerpop#gremlin-shaded;3.4.5 from spark-list in [default]',\n",
    "'org.apache.tinkerpop#tinkergraph-gremlin;3.4.5 from spark-list in [default]',\n",
    "'org.codehaus.groovy#groovy;2.5.7 from spark-list in [default]',\n",
    "'org.codehaus.groovy#groovy-json;2.5.7 from spark-list in [default]',\n",
    "'org.hdrhistogram#HdrHistogram;2.1.11 from spark-list in [default]',\n",
    "'org.javatuples#javatuples;1.2 from spark-list in [default]',\n",
    "'org.ow2.asm#asm;7.1 from spark-list in [default]',\n",
    "'org.ow2.asm#asm-analysis;7.1 from spark-list in [default]',\n",
    "'org.ow2.asm#asm-commons;7.1 from spark-list in [default]',\n",
    "'org.ow2.asm#asm-tree;7.1 from spark-list in [default]',\n",
    "'org.ow2.asm#asm-util;7.1 from spark-list in [default]',\n",
    "'org.reactivestreams#reactive-streams;1.0.2 from spark-list in [default]',\n",
    "'org.scala-lang#scala-reflect;2.11.12 from central in [default]',\n",
    "'org.slf4j#jcl-over-slf4j;1.7.25 from spark-list in [default]',\n",
    "'org.slf4j#slf4j-api;1.7.26 from spark-list in [default]',\n",
    "'org.yaml#snakeyaml;1.15 from spark-list in [default]',\n",
    "':: evicted modules:',\n",
    "'org.apache.commons#commons-lang3;3.8.1 by [org.apache.commons#commons-lang3;3.5] in [default]',\n",
    "'org.scala-lang#scala-reflect;2.11.8 by [org.scala-lang#scala-reflect;2.11.12] in [default]',\n",
    "'org.slf4j#slf4j-api;1.7.21 by [org.slf4j#slf4j-api;1.7.26] in [default]',\n",
    "'---------------------------------------------------------------------',\n",
    "'|                  |            modules            ||   artifacts   |',\n",
    "'|       conf       | number| search|dwnlded|evicted|| number|dwnlded|',\n",
    "'---------------------------------------------------------------------',\n",
    "'|      default     |   50  |   0   |   0   |   3   ||   47  |   0   |',\n",
    "'---------------------------------------------------------------------',\n",
    "':: retrieving :: org.apache.spark#spark-submit-parent-c122f657-bad9-405f-87b4-89a7dc1ef8e7',\n",
    "'confs: [default]',\n",
    "'0 artifacts copied, 47 already retrieved (0kB/16ms)',\n",
    "'20/05/23 12:38:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable',\n",
    "'Setting default log level to \"WARN\".',\n",
    "'To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).',\n",
    "'20/05/23 12:38:28 WARN util.Utils: Service \"SparkUI\" could not bind on port 4040. Attempting port 4041.',\n",
    "'20/05/23 12:38:28 WARN util.Utils: Service \"SparkUI\" could not bind on port 4041. Attempting port 4042.',\n",
    "'20/05/23 12:38:28 WARN util.Utils: Service \"SparkUI\" could not bind on port 4042. Attempting port 4043.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_producer = KafkaProducer(bootstrap_servers=['10.0.0.9:9092'],\n",
    "                         value_serializer=lambda data: data.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for log in log_data:\n",
    "    string_producer.send('topic_one', value=log)\n",
    "    counter += 1\n",
    "    if counter == 30:\n",
    "        break\n",
    "    sleep(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
