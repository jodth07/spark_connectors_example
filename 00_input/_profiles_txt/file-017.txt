David Dubois    410-858-4313  |  duboisdavid582@gmail.com



David

Dubois

HADOOP DATA ENGINEER





30 Years – Software Engineering

7 Years – Big Data Development

Profile

Software Engineer with 30+ years of experience developing software for operational systems, product analysis, big data analysis, real time applications, signals analysis, network solutions, and image processing.

Energetic and enthusiastic with 7+ years’ experience as a data professional with a passion for data processing, systems, data cleaning and preparing big data for use in various analytics.  Experience designing, architecting implementing and using Hadoop big data ecosystems.  Proficient in ETL and data pipeline methods and tools, and querying and cleaning data.  Key tools are: Hadoop, Spark, Amazon AWS, Cloudera, Hortonworks.



Professional Summary

Well-rounded, forward-thinking, and a highly motivated, self-starter with excellent communication and presentation skills. Known for creative insights and creative solutions, and able to adapt to changing environments and needs.  A natural leader, and an active, contributing member of any organization, helping people to work together to achieve common goals.

7+ years of experience in Hadoop-Big Data Engineering, Data Analytics, Data Processing and Database Technologies, database and systems administration.

Experience in data and batch migration to Hadoop, and Hands on experience in installing, configuring Cloudera's and Horton distribution.

Excellent knowledge on Hadoop Architecture and ecosystems such as HDFS, configuration of nodes, YARN, Sentry, Spark, Falcon, Hbase, Hive, Pig, Sentry, Ranger.

Incremental imports, partitioning and bucketing concepts in Hive and Spark SQL needed for optimization.

Experience developing Oozie workflows for scheduling and orchestrating the ETL process.

Working with large complex data sets, real-time/near real-time analytics, and distributed big data platforms.

Experience deploying large multiple nodes of a Hadoop and Spark cluster.

Experience collecting log data from various sources and integrating it into HDFS using Flume; staging data in HDFS for further analysis.

Worked on disaster management with Hadoop cluster, and Involved in building a multi-tenant cluster.

Ability to troubleshoot and tune relevant programming languages like SQL, Python, Scala, PIG, Hive, RDDs, DataFrames. Able to design elegant solutions through the use of problem statements.

Strong hands-on experience in Hadoop; HDFS Architecture, Hive, Pig, Sqoop, HBase, MongoDB, Cassandra, Oozie, Spark RDDs, Spark DataFrames, Spark Datasets,




Technical Skills	

	Programming & Scripting 	

20+ years of experience: C++, Linux/Unix shell scripting, Perl, Java, Python, SQL, MySQL, NoSQL, HTML5, CSS3, Visual Basic, Hive QL, Python, Scala, Cobol, XML, Blueprint XML, Ajax, REST API, Spark API, JSON, Avro, Parquet, ORC, Jupyter Notebooks, Eclipse, IntelliJ, PyCharm

	Software Development 	

Experienced in the full software development life cycle from requirements gathering through architecture, development, testing and deployment.  Expertise in the development of UI/UX design and implementation -  interfaces including graphical trees and other detailed data with data analysis to present the operator with red, green, yellow type statuses. Software Design, Integration, Productivity tool development.  Database scripting and product count analysis to simplify the visualization of the data to an operator. Developed scripts to read the delivery status from the database and developed a clear color-coded display and summary that because the Product Tracker application. 

	Data & File Management	

SQL and NoSQL, RDBMS, Apache Cassandra, Apache HBase, MapR-DB, MongoDB, Oracle, SQL Server, DB2, Sybase, RDBMS, HDFS, Parquet, Avro, JSON, Snappy, Gzip, DAS, NAS, SAN, Data Analysis and Reporting, Skilled in Digital reporting, dashboards, and making presentations.

	Hadoop Distributions	

	Cloudera Hadoop and Hortonworks Hadoop

	Hadoop Big Data	

Apache Ant, Apache Cassandra, Apache Flume, Apache Hadoop, Apache Hadoop YARN, Apache HBase, Apache HCatalog, Apache Hive, Apache Kafka, Apache MAVEN, Apache Oozie, Apache Pig, Apache Spark, Spark Streaming, Spark MLlib, GraphX, SciPy, Pandas, RDDs, DataFrames, Datasets, Mesos, Apache Tez, Apache ZooKeeper, Cloudera Impala, HDFS, Hortonworks, Apache Airflow and Camel, Apache Lucene, Elasticsearch, Elastic Cloud, Kibana, X-Pack, Apache SOLR, Apache Drill, Presto, Apache Hue, Sqoop, Kibana, Tableau, AWS, Cloud Foundry, AWS, Azure, Anaconda Cloud, Elasticsearch, Solr, Lucene, Databricks, MapReduce

	Soft Skills	

	Natural Leader, involved in the community, mentoring and advocacy for others, written and verbal communication, presentation and reporting highly motivated, self-starter, strong sense of ethics and helping others, strong interpersonal skills, communication skills, highly effective time management and productivity., hard-working, generous and fair-minded.  Skilled in analysis, critical thinking, evaluation and creative, custom solutions.

Professional Experience

		

		May 2016	Present

		Hadoop Data Architect/Engineer

		Harris Corporation – Columbia, MD

Worked with a team of developers analyzing various large data sets for trends and insider threat analysis using Hadoop, machine learning, and predictive analytics.

Created a data pipeline in both on promise and cloud projection environment such as AWS.

Designed AWS architecture of the big data ecosystem per client request 

Managed credential of users in AWS from IAM role

Utilized Spark in EMR and EC2 and processed the data to the S3 and Redshift 

Worked on disaster management with Hadoop cluster.

Collect, aggregate, and move data from servers to HDFS using Apache Spark & Spark Streaming.

Data ingestion is done using Flume with source as Kafka Source & sink as HDFS.

Created Hive external tables and designed data models in hive. 

For one of the use case, used Spark Streaming with Kafka & HDFS & MongoDB to build a continuous ETL pipeline. This is used for real-time analytics performed on the data. 

Configured Spark streaming to receive real-time data from Kafka and store the stream data to HDFS. 

Involved in the process of designing Cassandra Architecture including data modeling.

Performance tuning of HIVE service for better Query performance on ad-hoc queries.

Integrated Hadoop with Active Directory and enabled Kerberos for Authentication.

Administered Hadoop cluster(CDH) and reviewed log files of all daemons.

Performed import and export of dataset transfer between traditional databases and HDFS using Sqoop.

Used Spark SQL and DataFrames API to load structured and semi structured data into Spark Clusters.

Used Impala where possible to achieve faster results compared to Hive during data Analysis.

Performed storage capacity management, performance tuning and benchmarking of clusters.

Implemented YARN Resource pools to share resources of cluster for YARN jobs submitted by users.

Migrated ETL jobs to Pig scripts for transformations, joins, aggregations before HDFS.

Implemented workflows using Apache Oozie framework to automate tasks.

Performed both major and minor upgrades to the existing Cloudera Hadoop cluster.

Experience in optimizing the data storage in Hive using partitioning and bucketing mechanisms on both the managed and external tables.

Performed performance tuning for Spark Steaming e.g. setting right Batch Interval time, correct level of Parallelism, selection of correct Serialization & memory tuning.

Implemented data ingestion and cluster handling in real-time processing using Kafka. 

Worked on importing and exporting data using Sqoop between HDFS to RDBMS.

Used Spark API over Hadoop YARN to perform analytics on data in Hive.

Designed and presented a POC on introducing Impala in project architecture.

Implemented High Availability of Name Node, Resource manager on the Hadoop Cluster.

Involved in creating Hive Tables, loading with data and writing Hive queries.



Environment: HDFS, PIG, Hive, Sqoop, Oozie, HBase, Zoo keeper, Cloudera Manager, Ambari, Oracle, MYSQL, Cassandra, Sentry, Falcon, Spark, YARN






		May 2015 	May 2016

		Hadoop Data Architect/Engineer

		Zurich N.A. – Schaumburg, IL

Zurich N.A. uses Hadoop for big data analytics to query customer activity in real time.  Using advanced analytics on data streams like advanced windowing, event correlation, event clustering, anomaly detection, and so on, play a large role in the financial giant's corporate strategy and administration of accounts, products, investments, and customers. 

Implemented YARN Resource pools to share resources of cluster for YARN jobs submitted by users.

Involved in creating Hive Tables, loading with data and writing Hive queries

Migrated ETL jobs to Pig scripts for transformations, joins, aggregations before HDFS.

Used Impala where possible to achieve faster results compared to Hive during data Analysis.

Performed both major and minor upgrades to the existing Cloudera Hadoop cluster.

Involved in the process of designing Cassandra Architecture including data modeling.

Used Spark SQL and DataFrames API to load structured and semi-structured data into Spark Clusters. 

Implemented data ingestion and cluster handling in real-time processing using Kafka. 

Implemented workflows using Apache Oozie framework to automate tasks.

Implemented High Availability of Name Node, Resource manager on the Hadoop Cluster.

Performed performance tuning for Spark Steaming e.g. setting right Batch Interval time, correct level of Parallelism, selection of correct Serialization & memory tuning.

Collect, aggregate, and move data from servers to HDFS using Apache Spark & Spark Streaming.

Performance tuning of HIVE service for better Query performance on ad-hoc queries.

Performed storage capacity management, performance tuning and benchmarking of clusters.

Created Hive external tables and designed data models in hive. 

Integrated Hadoop with Active Directory and enabled Kerberos for Authentication.

Performed import and export of dataset transfer between traditional databases and HDFS using Sqoop.

Data ingestion is done using Flume with source as Kafka Source & sink as HDFS.

Experience in optimizing the data storage in Hive using partitioning and bucketing mechanisms on both the managed and external tables.

Worked on disaster management with Hadoop cluster.

Administered Hadoop cluster(CDH) and reviewed log files of all daemons.

Migrated complex programs into Apache Spark RDD operations.

Worked on importing and exporting data using Sqoop between HDFS to RDBMS.

For one of the use case, used Spark Streaming with Kafka & HDFS & MongoDB to build a continuous ETL pipeline. This is used for real-time analytics performed on the data. 

Used Spark API over Hadoop YARN to perform analytics on data in Hive. Imported data into HDFS and Hive using Sqoop and Kafka. Created Kafka topics and distributed to different consumer applications.

Worked on Spark SQL and DataFrames for faster execution of Hive queries using Spark and AWS EMR 

Implemented Partitioning, Dynamic Partitions and Buckets in HIVE for increasing performance benefit and helping in organizing data in a logical fashion. 

Scheduled and executed workflows in Oozie to run Hive and Pig jobs 

Worked with Spark Context, Spark -SQL, DataFrame and Pair RDDs.

Used Hive, spark SQL Connection to generate Tableau BI reports.

Created Partitions, Buckets based on State to further process using Bucket based Hive joins.

Created Hive Generic UDF's to process business logic that varies based on policy.

Developed various data connections from data sourced to SSIS, and Tableau Server for report and dashboard development.

Worked with clients to better understand their reporting and dashboarding needs and present solutions using structured Waterfall and Agile project methodology approach. 

Developed metrics, attributes, filters, reports, dashboards and also created advanced chart types, visualizations and complex calculations to manipulate the data.

Environment: Hadoop, HDFS, Hive, Spark, YARN, Kafka, Pig, MongoDB, Sqoop, Storm, Cloudera, Impala 





		

		Jan 2014	May 2015

		Hadoop Data Engineer

		TechData – Clearwater FL

Credit card companies are data savvy organizations basing operations and strategy on enormous numbers of credit card holders and merchants.  This project is based on Capital One’s analysis of credit profiles and user patterns in predictive analytics to manage risk and to attract the best and most lucrative customers.

Deployed the application jar files into AWS instances.

Used the image files of an instance to create instances containing Hadoop installed and running.

Developed a task execution framework on EC2 instances using SQL and DynamoDB.

Designed a cost-effective archival platform for storing big data using Hadoop and its related technologies. 

Connected various data centers and transferred data between them using Sqoop and various ETL tools.

Extracted the data from RDBMS (Oracle, MySQL) to HDFS using Sqoop. 

Used the Hive JDBC to verify the data stored in the Hadoop cluster.

Worked with the client to reduce churn rate, read and translate data from social media websites.

Integrated Kafka with Spark Streaming for real-time data processing 

Imported data from disparate sources into Spark RDD for processing.

Built a prototype for real-time analysis using Spark streaming and Kafka.

Transferred data using Informatica tool from AWS S3.

Using AWS Redshift for storing the data on cloud.

Collected the business requirements from the subject matter experts like data scientists and business partners.

Involved in Design and Development of technical specifications using Hadoop technologies.

Load and transform large sets of structured, semi-structured and unstructured data. 

Used different file formats like Text files, Sequence Files, Avro.

Loaded data from various data sources into HDFS using Kafka.

Tuning and operating Spark and its related technologies like Spark SQL and Streaming.

Used shell scripts to dump the data from MySQL to HDFS.

Used NoSQL databases like MongoDB in implementation and integration.

Worked on streaming the analyzed data to Hive Tables using Sqoop for making it available for visualization and report generation by the BI team.

Configured Oozie workflow engine scheduler to run multiple Hive, Sqoop and pig jobs.

Consumed the data from Kafka queue using Storm

Used Oozie to automate/schedule business workflows which invoke Sqoop, and Pig jobs as per the requirements.



Environment:  Hadoop, Spark, HDF, Oozie, Sqoop, MongoDB, Hive, Pig, Storm, Kafka, SQL, Acro, RDD. SQS S3, Cloud, MySQL, Informatica, Dynamo DB





		Aug 2010	Dec 2013

		Hadoop Data Developer

		Open Lending – Austin, TX

Gartner is a leading global research and advisory company.  Gartner uses Hadoop big data analytics to help business leaders across all major functions and industries make the right decision using objective and accurate insights gathered from real-time data. 

Moving data from Oracle to HDFS and vice-versa using SQOOP.

Worked on installing cluster, commissioning and decommissioning of data node, NameNode recovery, capacity planning, and slots configuration.

Used Oozie Scheduler system to automate the pipeline workflow and orchestrate extraction of data.

Documented requirements gathered from stakeholders. 

Collected and aggregated large amounts of log data using Apache Flume and staging data in HDFS for further analysis.

Worked with different file formats and compression techniques to determine standards.

Involved in loading data from Linux file system to HDFS.

Used Linux shell scripts to automate the build process, and to perform regular jobs like file transfers between different hosts.

Involved in loading the created Files into HBase for faster access of all the products in all the stores without taking Performance hit.

Imported data using Sqoop to load data from MySQL and Oracle to HDFS on regular basis.

Installed and configured Pig for ETL jobs and made sure we had Pig scripts with regular expression for data cleaning.

Successfully loaded files to HDFS from Teradata, and loaded from HDFS to HIVE.

Used Zookeeper and Oozie for coordinating the cluster and scheduling workflows.

Creating Hive external tables to store the Pig script output. Working on them for data analysis in order to meet the business requirements.

Used Sqoop to efficiently transfer data between databases and HDFS and used Flume to stream the log data from servers.

Used Zookeeper for providing coordinating services to the cluster.

Implemented partitioning, bucketing in Hive for better organization of the data.

Documented Technical Specs, Dataflow, Data Models and Class Models.

Responsible for building scalable distributed data solutions using Hadoop.

Involved in production support, which involved monitoring server and error logs, and foreseeing and preventing potential issues, and escalating issue when necessary.

Environment: Hadoop Cluster, HDFS, Hive, Pig, Sqoop, Linux, HBase, Shell Scripting, Eclipse, Oozie, Navigator.



		Nov 2007	Aug 2010

		Software Engineer III

		Intellicast – Baltimore, MD



Worked on the DOST team (day in the life of the GOES ground station).

Performed on console work monitoring the operational system for both GOES weather satellites.

Provided Productivity Tool Development and Engineering Support to the GOES Data Operations Support Team (DOST), a Tiger Team tasked with operationalizing the downstream data processing and distribution of geostationary weather products.

Monitored the system / looking at the algorithm output products for image quality and frequency of the products. Performed overall product counts to verify the ground system was running as expected (Python).

Analyzed the feasibility of using the existing tools to monitor the ground system was untenable, sought new tools to fill in the gaps and investigated the database for information that could supply more information to an operator (MySQL).

Developed scripts to read the delivery status from the database and display the results to an operator in a more concise, clear, color coded manner. This effort turned into the Product Tracker which provides the operator with both a graphical summary and delivery details that the operator uses to verify the delivery of products to all of its external interfaces (Python and bash).

Developed image down-sampling software to output JPEG images at 100-megabyte vs the original size of 2 gigabytes allowing an operator to perform a quality check of the products in 5 minutes instead of 5 hours (Python).



		April 2005	Nov 2007

		Software Engineer III

		Harris Corporation – Annapolis Junction, MD

Worked with a team of developers analyzing various large data sets for trends and insider threat analysis using Hadoop, machine learning, and predictive analytics.



		Feb 2003	April 2005

		Software Engineer III

		Harris Corporation – Annapolis Junction, MD

Worked with a team of developers analyzing various large data sets for trends and insider threat analysis using Hadoop, machine learning, and predictive analytics.



		Nov 2000	Feb 2003

		Software Engineer III

		Harris Corporation – Greenbelt, MD

Software development for the GOES-R program. Worked with the team of developers across the country to perform requirements analysis, UML design, develop, and test C++ and Java code integrating GOES-R science algorithms into PGIM. Also acting as the deputy lead for the L2 team.



		July 1995	Oct 2000

		Software Engineer III

		Harris Corporation – Long Beach, CA

Software and Hardware integration for the Hunter program. Worked with the team of developers across the country to integrate C/C++ code, GOTS and COTS hardware, Ethernet, KGs, ATM, and HIPPI interfaces.

Software integration for the GMR program encryption/decryption algorithms, key management, certificate signing, and radio control software for the JTRS GMR radio.

Modeling and simulation: performed orbit analysis and modeled communications paths both bent pipe and multi hop.



		March 1990	July 1995

		Software Engineer III

		Harris Corporation – Melbourne, FL

Developed software for an IR&D to monitor and control a large control system and performed automatic fault analysis on the results.

Developed image processing software for both MET and ORIGIN. 

Developed 3D modeling software (RealSite); utilizing this software and DigitalGlobe imagery developed a 3D model of Baghdad and other areas for the army.

Worked with a team in the development of automatic extraction of detailed 3D data from 2D images which resulted in a new patent.

Developed software for RaptorMist to exploit weaknesses in waveforms. Developed a demo which was conducted over a several hundred-mile-area to show our results.



		Jan 1985	Feb 1990

		Software Engineer II

		Lockheed Martin – Orlando, FL

Developed software for signal processing and analysis systems including some embedded devices where there was no OS. A number of these applications where developed using table driven software to enhance maintainability.

	

Education



Bachelor of Science, Computer Science (BSCS)

University Of Central Florida – Orlando



Hadoop training

Cloudera, Columbia, MD



Security Clearance

Personal Trust Clearance with the DOJ for NOAA



Certifications

IBM – Big Data 101

IBM – Hadoop 101

IBM – Moving Data into Hadoop