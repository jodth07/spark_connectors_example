Hadoop Engineer

Big Data Engineer

Hadoop Engineer

Big Data Engineer

Ph: (999) 999-9999

E: consultant@gmail.com

Ph: (999) 999-9999

E: consultant@gmail.com









Jeremie Havice

Big Data Engineer

Jeremie Havice

Big Data Engineer





 PROFESSIONAL SUMMARY  



Jeremie Havice

Big Data Engineer

Ph: (999) 999-9999

E: consultant@gmail.com

5 years of experience in the IT field

5 years in Big Data Engineering

Hadoop, Cloudera, Hortonworks, AWS

Spark, Scala, Kafka, HDFS, Hive



Experience in distributed big data systems with Hadoop, AWS EMR Hadoop clusters

Primary technical skills in HDFS, Spark, Kafka, Hive, Sqoop, HBase, Flume, Oozie, Zookeeper, YARN.

Skilled in Hadoop big data using Cloudera (CDH) and Hortonworks (HDP) 

Experienced team lead providing mentoring to engineers, and liaison for team with stakeholders, business units, data scientists/analysts and making sure all teams collaborate smoothly. 

Have good experience in extracting and generating data visualizations.

Facilitation of meetings following Scrum processes such as Sprint Planning, Backlog, Sprint Retrospective, Requirements Gathering and providing planning and documentation for project; ensuring project is on track with stakeholder wishes.

In depth understanding/knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node and MapReduce concepts and experience in working with

Used Apache Hadoop for working with Big Data to analyze large data sets efficiently.

Hands on experience in working with Ecosystems like Hive, Sqoop, MapReduce, Oozie. 

Hive's analytical functions, extending Hive core functionality by writing custom SQL queries.

Experience in importing and exporting Terabytes of data between HDFS and Relational Database Systems using Sqoop.

ETL from databases such as SQL Server and Oracle to Hadoop HDFS in Data Lake.

Experience in writing SQL queries, Stored Procedures, Triggers, Cursors and Packages.

Experience in handling XML files and related technologies.








 TECHNICAL SKILLS SUMMARY  

Programming & Scripting

█    █    █    █    █    █    █    █    █    █

Java, Scala, Python, Unix Shell Scripting

Data & File Management

█    █    █    █    █    █    █    █    █    █

Apache Cassandra, Apache Hbase, Oracle, SQL Server, RDBMS, HDFS, Parquet, Avro, JSON, Snappy, Gzip

Methodologies

█    █    █    █    █    █    █    █    █    █

Agile, Kanban, Scrum, Jira, Continuous Integration (CI CD), Test-Driven Development, Unit Testing, 

Big Data ETL Tools & Frameworks

█    █    █    █    █    █    █    █    █    █

	Hive, MapReduce,  Sqoop, Kafka, Spark, ELK Stack (Elasticsearch, Logstash, Kibana)

Big Data Platforms & Distributions

█    █    █    █    █    █    █    █    █    █

Hadoop, Cloudera Hadoop (CDH), Hortonworks Hadoop Data Platform (HDP)

Hadoop Cluster Security

█    █    █    █    █    █    █    █    █    █

Security:  Kerberos, Ranger

Big Data Visualization

█    █    █    █    █    █    █    █    █    █

Visualization:  Kibana, Tableau



Soft Skills

█    █    █    █    █    █    █    █    █    █

Leadership, Adaptability, Self-Motivation, Mentoring, Communication, Team Lead, Team Work





 PROFESSIONAL EXPEREINCE  

Senior Big Data Engineer April 2017 – Present

TechField Atlanta, GA



Created an application that collects data that can be analyzed to show how the company is received in the general public.  Was able to collect tweets that mentioned the companies name or any like terms and any spatial data.



Installation and configuration of the various Big Data ecosystem tools such as Elastic Search, Logstash, Kibana, Kafka and Cassandra.

Built Real-Time Streaming Data Pipelines with Kafka, Spark Streaming and Cassandra.

Implemented Spark streaming for real-time data processing with Kafka and handled large amounts of data with Spark.

Wrote streaming applications with Spark Streaming/Kafka. 

Used Spark SQL to perform transformations and actions on data residing in HDFS.

Responsible for designing and deploying new ELK clusters.

Worked with Elasticsearch and Logstash (ELK) performance and configure tuning.

Created a Kafka producer to connect to different external sources and bring the data to a Kafka broker.

Handled schema changes in data stream using Kafka.

Analyzed and tuned Cassandra data model for multiple internal projects/customers.

Developed ETL pipeline to process log data from Kafka/HDFS sequence file and output to Hive tables in ORC format.

Built Jenkins jobs for CI/CD infrastructure from GitHub repos.

Support for the clusters, topics on the Kafka manager.

Responsible for Kafka operation and monitoring, and handling of messages funneled through Kafka topics.

Coordinated Kafka operation and monitoring with dev ops personnel; formulated balancing impact of Kafka producer and Kafka consumer message(topic) consumption.

Versioning with Git and set-up a Jenkins CI to manage CICD practices.

Pulled data and populated the data in Kibana.

Kibana dashboard designed over Elasticsearch for visualizing the data.

Interacted with data residing in HDFS using Spark to process the data.

Participated in various phases of data processing (collecting, aggregating, moving from various sources) using Apache Spark.

Handled structured data via Spark SQL then stored into Hive tables for downstream consumption.

Worked with Elasticsearch and Logstash (ELK) performance and configure tuning.



Big Data Developer July 2015 – Jan 2017

Kellog Brown & Root (KBR)

Houston, Tx



Implemented Serverless architecture using AWS Lambda with Amazon S3 and Amazon Dynamo DB. 

Expertise in AWS data migration between different database platforms like Local SQL Server to Amazon RDS and EMR HIVE.

Led many critical on-prem data migration to AWS cloud, assisting the performance tuning and providing successful path towards Redshift Cluster and AWS RDS DB engines.

Worked on AWS S3 bucket integration for application and development projects.

Experience in managing and reviewing Hadoop log files in AWS S3. 

Responsible for Designing Logical and Physical data modelling for various data sources on Confidential Amazon Redshift.

Installed, configured, and tested an AWS Lambda function workflow in Python

Responsible for continuous monitoring and managing Elastic MapReduce (EMR) cluster through AWS console.

Automated and defined Spark and AWS Best practices for future deployment.

Worked on Multiple AWS instances, set the security groups, Elastic Load Balancer and AMIs, Auto scaling to design cost effective, fault tolerant and highly available systems on AWS.

Deploy Spark Jobs into AWS EMR.

Managed AWS Redshift clusters such as launching the cluster by specifying the nodes and performing the data analysis queries.

Created basic infrastructure of the pipeline using AWS Cloud Formation.

Implemented usage of Amazon EMR for processing Big Data across Hadoop Cluster on AWS.

Managed and monitored AWS EC2 instances through AWS Management Console

Participated into the AWS architecture, design and planning from ingestion into reporting.

Securely controlling AWS users and groups access to AWS services and resources by assigning roles and polices using IAM.

Configured Elastic search, Log stash and Kibana (ELK) for log analytics, full text search, application monitoring in integration with AWS Lambda and Cloud Watch.

Configuring Access for inbound and outbound traffic RDS DB services, DynamoDB tables, EBS volumes to set alarms for notifications or automated actions on AWS.



Only have 3 Companies!!!

Experienced collecting real-time log data from different sources like webservers and social media using Flume, and storing in HDFS for further analysis.

Loaded data into HBase tables and Hive tables consumption purposes.

Optimization of Hive tables and large sets of structured, semi structured, and unstructured data.

Hive partitioning, bucketing, performing joins on Hive tables.

Installed Oozie workflow engine to run multiple jobs with Hive HQL.

Hands-on experience in working with job scheduling with Oozie.

Partitioning and bucketing to optimize Hive app.

Used HiveQL scripts to create and load data into diverse Hive tables

Worked with over 100 terabytes of data from data warehouse and over 1 petabyte of data from Hadoop cluster.

Job management using Fair Scheduler, and development of job processing scripts using Oozie workflow to run multiple Spark Jobs in sequence for processing data.

Responsible for performance optimization of clusters.

Hadoop Cloudera Platform (Hive,HDFS & Spark).

Cloudera Hadoop distribution version CDH5 for executing the respective scripts.

Created Hive Generic UDF's to process business logic.

Imported/exported from various sources to HDFS to build Data Lake.

Developed Oozie workflow for scheduling and orchestrating the ETL process within the Cloudera Hadoop system.

Deep understanding and implementations of various methods to load HIVE tables from HDFS and Local File System.

Wrote incremental imports into Hive tables.

Created Hive tables to store the processed results in a tabular format.

Installed Oozie workflow engine to run multiple Hive Jobs.

	

	Hadoop Developer 	Feb 2014 – July 2015

ITT Exelis 

Tysons Corner, VA

Used Zookeeper and Oozie for coordinating the cluster and scheduling workflows.

Ambari to monitor workload, job performance and capacity planning.

Managing Hadoop clusters via Command Line, and Hortonworks Ambari agent.

Performed cluster and system performance tuning.

Monitored Hadoop cluster using tools like Ambari.

Cluster coordination services through Zookeeper and Kafka.

Worked on resolving RANGER and Kerberos issues.

Configure Yarn capacity scheduler to support various business SLA's.

Enabled security to the cluster using Kerberos and integrated clusters with LDAP at Enterprise level.

Implement and maintain security LDAP, Kerberos as designed for cluster.

Coordinates with monitors cluster upgrade needs, and monitors cluster health and builds proactive tools to look for anomalous behaviors.

Worked with cluster users to ensure efficient resource usage in the cluster and alleviate multi-tenancy concerns.

Set-up Kerberos for more advanced security features for users and groups.

Implemented enterprise security measures on big data products including HDFS encryption/Apache Ranger Managing and Scheduling batch jobs on a Hadoop Cluster using Oozie.

Worked on Kafka cluster environment and zookeeper.

Monitored multiple Hadoop clusters environments using Ambari.

Experience in configuring, installing and managing Hortonworks (HDP) Distributions.

Involved in implementing security on HDP Hadoop Clusters with Kerberos for authentication and Ranger for authorization and LDAP integration for Ambari, Ranger

Secured the Kafka cluster with Kerberos.





 EDUCATION

B.S. in Computer Science

Kansas State University

Manhattan, Kansas



Associates of Applied Science With Honors

Cloud County Community College

Junction City, Kansas



Honors

Phi Theta Kappa Fraternal Order of Honor Students